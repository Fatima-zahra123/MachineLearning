{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ff0b268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionnaire_des_mots: {'la': 4, 'vie': 8, 'est': 2, 'douce': 1, 'tranquille': 7, 'ou': 5, 'belle': 0, 'hello': 3, 'rihab': 6}\n",
      "liste_des_mots: ['la', 'vie', 'est', 'douce', 'tranquille', 'ou', 'belle', 'hello', 'rihab']\n",
      "Matrice_sparse_correspondante:\n",
      " [[0 1 1 0 1 0 0 0 1]\n",
      " [1 1 3 0 1 1 0 1 1]\n",
      " [0 0 0 1 0 0 1 0 0]]\n",
      "  (0, 4)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 2)\t3\n",
      "  (1, 1)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 0)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 6)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer #importer une classe CountVectorizer et une package text\n",
    "texte = [\"La vie est douce\",\"La vie est tranquille,est ou belle,est douce\",\"hello Rihab\"] #une liste (vecteur) de deux texts\n",
    "vect = CountVectorizer() #instantiation \n",
    "T=vect.fit_transform(texte) #count how many times each word existed in the given data set\n",
    "dictionnaire_des_mots=vect.vocabulary_ #attribut de la classe vect ( CountVectorizer)\n",
    "print(\"dictionnaire_des_mots:\",dictionnaire_des_mots)\n",
    "liste_des_mots=list(dictionnaire_des_mots.keys()) #retourner les motes sans valeurs sous forme d'une liste\n",
    "print(\"liste_des_mots:\",liste_des_mots)\n",
    "Matrice_sparse_correspondante=T.toarray() \n",
    "print(\"Matrice_sparse_correspondante:\\n\",Matrice_sparse_correspondante)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47aea7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\T460s\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeff6673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vie rose', 'la foret y a lion', 'vie tres douce']\n"
     ]
    }
   ],
   "source": [
    "def netoyage(corpus_ensemble_documents):\n",
    "    for i in range(len(corpus_ensemble_documents)):\n",
    "        corpus_ensemble_documents[i]=corpus_ensemble_documents[i].lower()\n",
    "    for i in range(len(corpus_ensemble_documents)):\n",
    "        for c in string.punctuation:\n",
    "            x=corpus_ensemble_documents[i].replace(c,'')\n",
    "            corpus_ensemble_documents[i]=x\n",
    "    stopwords_anglais = stopwords.words('french') #ou french\n",
    "    for i in range(len(corpus_ensemble_documents)):\n",
    "        L = corpus_ensemble_documents[i].split()\n",
    "        for mot in L:\n",
    "            if mot in stopwords_anglais:\n",
    "                while mot in L:\n",
    "                    L.remove(mot)\n",
    "        corpus_ensemble_documents[i] = \" \".join(L)\n",
    "    return(corpus_ensemble_documents) \n",
    "print(netoyage([\"La vie en rose?\",\"  dans la Foret il y a un Lion ...\",\".. la vie est tres douce!\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1ea87b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exo spams\n",
    "\n",
    "import numpy \n",
    "def TF(terme,corpus,numero_document):\n",
    "    x=corpus[numero_document].count(terme)\n",
    "    y=len(corpus[numero_document].split())\n",
    "    return x/y\n",
    "def IDF(terme,corpus,numero_document):\n",
    "    D=len(corpus)\n",
    "    d_t=0\n",
    "    for document in corpus:\n",
    "        if terme in document:\n",
    "            d_t+=1\n",
    "    TF_VAL=TF(terme,corpus,numero_document)\n",
    "    return TF_VAL*log(1+(D/d_t))\n",
    "def cles_correspondante_a_valeur(valeur,dictionnaire):\n",
    "    for cle in dictionnaire.keys():\n",
    "        if dictionnaire[cle]==valeur:\n",
    "            return cle\n",
    "def matrice_sparse(dictionnaire,corpus_ensemble_documents):\n",
    "    M=numpy.zeros((len(corpus_ensemble_documents),len(dictionnaire.values())))\n",
    "    for i in range(len(corpus_ensemble_documents)):\n",
    "        for j in dictionnaire.values():\n",
    "            x=cles_correspondante_a_valeur(j,dictionnaire)\n",
    "            M[i,j]=IDF(x,corpus_ensemble_documents,i)\n",
    "    return M\n",
    "def affiche(M):\n",
    "    (n,p)=M.shape\n",
    "    for i in range(n):\n",
    "        for j in range(p):\n",
    "            M[i,j]=round(M[i,j],2)\n",
    "    print(M)  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "991c1e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionnaire_des_mots: {'vie': 5, 'douce': 2, 'tranquilleest': 4, 'belleest': 0, 'coronavirus': 1, 'méchant': 3}\n",
      "liste_des_mots: ['vie', 'douce', 'tranquilleest', 'belleest', 'coronavirus', 'méchant']\n",
      "Matrice_sparse_methode_predefinie:\n",
      "\n",
      "[[0.   0.   0.71 0.   0.   0.71]\n",
      " [0.56 0.   0.43 0.   0.56 0.43]\n",
      " [0.   0.71 0.   0.71 0.   0.  ]]\n",
      "Matrice_sparse obtenue par notre methode :\n",
      "\n",
      "[[0.   0.   0.46 0.   0.   0.46]\n",
      " [0.35 0.   0.23 0.   0.35 0.23]\n",
      " [0.   0.69 0.   0.69 0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer #importer une classe CountVectorizer et une package text\n",
    "import nltk\n",
    "texte = [\"La vie est douce\",\"La vie est tranquille,est ou belle,est douce\",\"le corona-virus est méchant\"] #une liste (vecteur) de deux texts\n",
    "texte=netoyage(texte)\n",
    "vect=TfidfVectorizer()\n",
    "T=vect.fit_transform(texte) #count how many times each word existed in the given data set\n",
    "dictionnaire_des_mots=vect.vocabulary_ #attribut de la classe vect ( CountVectorizer)\n",
    "print(\"dictionnaire_des_mots:\",dictionnaire_des_mots)\n",
    "liste_des_mots=list(dictionnaire_des_mots.keys()) #retourner les motes sans valeurs sous forme d'une liste\n",
    "print(\"liste_des_mots:\",liste_des_mots)\n",
    "Matrice_sparse_correspondante=T.toarray() \n",
    "print(\"Matrice_sparse_methode_predefinie:\\n\")\n",
    "affiche(Matrice_sparse_correspondante)\n",
    "print(\"Matrice_sparse obtenue par notre methode :\\n\")\n",
    "affiche(matrice_sparse(dictionnaire_des_mots,texte))\n",
    "\n",
    "\n",
    "#print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f036ab95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score sur data train:0.9976923076923077\n",
      "score sur data test:0.9814593301435407\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn.model_selection import train_test_split\n",
    "spams = pandas.read_table(\"C:\\SMSSpamCollection.txt\",sep=\"\\t\",header=0)\n",
    "spamsTrain,spamsTest = train_test_split(spams,train_size=0.7,random_state=1)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "parseur = CountVectorizer()\n",
    "XTrain = parseur.fit_transform(spamsTrain['message'])\n",
    "mdtTrain = XTrain.toarray()\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "modelFirst = LogisticRegression()\n",
    "#modelFisrt.fit(spamsTrain['message'],spamsTrain['classe'])\n",
    "modelFirst.fit(mdtTrain,spamsTrain['classe'])\n",
    "score1=modelFirst.score(mdtTrain,spamsTrain['classe'])\n",
    "print(\"score sur data train:\"+str(score1))\n",
    "mdtTest=parseur.transform(spamsTest['message'])\n",
    "#predTest=modelFirst.predict(mdtTest)\n",
    "score2=modelFirst.score(mdtTest,spamsTest['classe'])\n",
    "print(\"score sur data test:\"+str(score2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8a5ebfc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (Temp/ipykernel_4384/1471105588.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\T460s\\AppData\\Local\\Temp/ipykernel_4384/1471105588.py\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    spams = pandas.read_table(\"C:\\Users\\T460s\\Documents\\SMSSpamCollection.txt\" ,sep=\"\\t\",header=0)\u001b[0m\n\u001b[1;37m                                                                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "spams = pandas.read_table(\"C:\\Users\\T460s\\Documents\\SMSSpamCollection.txt\" ,sep=\"\\t\",header=0)\n",
    "spamsTrain,spamsTest = train_test_split(spams,train_size=0.7,random_state=1)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "parseur = CountVectorizer(binary=True)\n",
    "Xtrain = parseur.fit_transform(spamsTrain['message'])\n",
    "mdtTrain = XTrain.toarray()\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "modelFirst = LogisticRegression()\n",
    "#modelFisrt.fit(spamsTrain['message'],spamsTrain['classe'])\n",
    "modelFisrt.fit(mdtTrain,spamsTrain['classe'])\n",
    "score1=modelFirst.score(mdtTrain,spamsTrain['classe'])\n",
    "print(\"score sur data train:\"+str(score1))\n",
    "mdtTest=parseur.transform(spamsTest['message'])\n",
    "#predTest=modelFirst.predict(mdtTest)\n",
    "score2=modelFisrt.score(mdtTest,spamTest['classe'])\n",
    "print(\"score sur data test:\"+str(score2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a465e03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score sur data train:0.9953846153846154\n",
      "score sur data test:0.9760765550239234\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "parseurBis=CountVectorizer(stop_words='english')\n",
    "XTrainBis=parseurBis.fit_transform(spamsTrain['message'])\n",
    "mdtTrainBis=XTrainBis.toarray()\n",
    "modelBis=LogisticRegression()\n",
    "modelBis.fit(mdtTrainBis,spamsTrain['classe'])\n",
    "mdtTestBis=parseurBis.transform(spamsTest['message'])\n",
    "score3=modelBis.score(mdtTrainBis,spamsTrain['classe'])\n",
    "print(\"score sur data train:\"+str(score3))\n",
    "score4=modelBis.score(mdtTestBis,spamsTest['classe'])\n",
    "print(\"score sur data test:\"+str(score4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6560a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score sur data train:0.9743589743589743\n",
      "score sur data test:0.9712918660287081\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "parseur3=TfidfVectorizer()\n",
    "XTrain3=parseur3.fit_transform(spamsTrain['message'])\n",
    "mdtTrain3=XTrain3.toarray()\n",
    "model3=LogisticRegression()\n",
    "model3.fit(mdtTrain3,spamsTrain['classe'])\n",
    "mdtTest3=parseur3.transform(spamsTest['message'])\n",
    "score5=model3.score(mdtTrain3,spamsTrain['classe'])\n",
    "print(\"score sur data train:\"+str(score5))\n",
    "score6=model3.score(mdtTest3,spamsTest['classe'])\n",
    "print(\"score sur data test:\"+str(score6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a527c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
